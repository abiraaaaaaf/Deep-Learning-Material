{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "shape of t: torch.Size([4, 4])\n",
      "shape of t1: torch.Size([2, 8])\n",
      "shape of t2: torch.Size([2, 8])\n",
      "10.0\n",
      "tensor([[10.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones([4, 4])\n",
    "print(t)\n",
    "\n",
    "print('shape of t:', t.shape)\n",
    "\n",
    "t1 = t.view(2, 8)\n",
    "print('shape of t1:', t1.shape)\n",
    "\n",
    "t2 = t.view(-1, 8)\n",
    "print('shape of t2:', t2.shape)\n",
    "\n",
    "t1[0,0] = 10\n",
    "\n",
    "print(t[0, 0].item())\n",
    "\n",
    "print (t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.0272e+27, 4.5908e-41, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 6.0712e+27, 4.5908e-41],\n",
      "        [0.0000e+00, 0.0000e+00, 3.8983e-27, 4.5908e-41],\n",
      "        [0.0000e+00, 0.0000e+00,        nan, 0.0000e+00],\n",
      "        [1.4013e-45, 0.0000e+00, 1.4013e-45, 0.0000e+00]])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "a = torch.empty(5, 4, dtype=torch.float)\n",
    "print (a)\n",
    "print(a.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9606,  0.2111, -0.5098,  0.3270, -0.9663, -0.1154, -1.4709],\n",
      "        [ 1.2724,  1.2059,  0.3888, -0.0078,  1.5804, -1.3624, -1.3922],\n",
      "        [-0.4629,  1.4374, -0.7028,  1.1323,  0.5191, -1.1186, -1.5683],\n",
      "        [-1.0850, -1.5958,  0.0322, -0.5978,  0.1110,  0.4870, -1.3241],\n",
      "        [ 0.1471,  0.0996, -1.2445,  1.3746, -1.0573,  0.6674,  0.1917]],\n",
      "       dtype=torch.float64)\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(5, 7, dtype=torch.double)\n",
    "print(a)\n",
    "print(a.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000],\n",
      "        [7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000],\n",
      "        [7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000],\n",
      "        [7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000],\n",
      "        [7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a.fill_(3.5)\n",
    "b = a.add(4.0)\n",
    "# a.add_(4.0)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2438, -1.1430, -0.5280],\n",
      "        [ 0.1452, -1.2483, -0.0096]])\n",
      "tensor(-0.5280)\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,3, dtype=torch.float)\n",
    "print(a)\n",
    "c = a[0, 2]\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1430, -0.5280],\n",
      "        [-1.2483, -0.0096]])\n"
     ]
    }
   ],
   "source": [
    "c = a[0:2, 1:3]\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.4013e-43, 0.0000e+00],\n",
      "        [1.4013e-44, 0.0000e+00],\n",
      "        [1.1210e-43, 0.0000e+00],\n",
      "        [1.1210e-44, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00]])\n",
      "tensor([[ 10., 100.],\n",
      "        [ 10., 100.],\n",
      "        [ 10., 100.],\n",
      "        [ 10., 100.],\n",
      "        [ 10., 100.]])\n"
     ]
    }
   ],
   "source": [
    "z = torch.empty(5, 2)\n",
    "print(z)\n",
    "z[:, 0] = 10\n",
    "z[:, 1] = 100\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "tensor([[101.,   1.,   1.,   1.,  11.],\n",
      "        [101.,   1.,   1.,   1.,  11.],\n",
      "        [101.,   1.,   1.,   1.,  11.],\n",
      "        [101.,   1.,   1.,   1.,  11.],\n",
      "        [101.,   1.,   1.,   1.,  11.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(5, 5)\n",
    "print(x)\n",
    "\n",
    "x.index_add_(1, torch.tensor([4, 0], dtype=torch.long), z)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\torch\\cuda\\__init__.py:117: UserWarning: \n",
      "    Found GPU0 GeForce GT 750M which is of cuda capability 3.0.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn % (d, name, major, capability[1]))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (48) : no kernel image is available for execution on the device at c:\\a\\w\\1\\s\\windows\\pytorch\\aten\\src\\thc\\generic/THCTensorMath.cu:14",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-e59f4efa23ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuda:0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cuda runtime error (48) : no kernel image is available for execution on the device at c:\\a\\w\\1\\s\\windows\\pytorch\\aten\\src\\thc\\generic/THCTensorMath.cu:14"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "\n",
    "    a = torch.full((10,), 3, device=torch.device(\"cuda:0\"))\n",
    "    print(type(a))\n",
    "    b = a.to(torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3], dtype=torch.float32, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 6.], grad_fn=<MulBackward0>)\n",
      "tensor(12., grad_fn=<SumBackward0>)\n",
      "<MulBackward0 object at 0x000001B190FC1DD8>\n",
      "<SumBackward0 object at 0x000001B1A37E1278>\n"
     ]
    }
   ],
   "source": [
    "w = x * 2\n",
    "y = w.sum()\n",
    "\n",
    "print(w)\n",
    "print(y)\n",
    "\n",
    "print(w.grad_fn)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([2., 2., 2.])\n"
     ]
    }
   ],
   "source": [
    "print(w.grad)  # gradient is computed only w.r.t. graph leaves\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([2., 2., 2.])\n",
      "tensor([4., 4., 4.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3], dtype=torch.float32, requires_grad=True)\n",
    "w = x * 2\n",
    "y = w.sum()\n",
    "\n",
    "y.backward(retain_graph=True)\n",
    "\n",
    "print(w.grad)  # gradient is computed only w.r.t. graph leaves\n",
    "print(x.grad)\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(x.grad) # accumulating gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2.])\n",
      "tensor([4., 4., 4.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3], dtype=torch.float32, requires_grad=True)\n",
    "w = x * 2\n",
    "y = w.sum()\n",
    "\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "\n",
    "w = x * 2\n",
    "y = w.sum()\n",
    "\n",
    "y.backward()\n",
    "print(x.grad)  # accumulating gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locally disabling gradient computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    w = x * 2\n",
    "    y = w.sum()\n",
    "    \n",
    "print(y.requires_grad)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### detach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], requires_grad=True)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-898f8534093c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m##error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1,2,3], dtype=torch.float32, requires_grad=True)\n",
    "print(x)\n",
    "print(x.numpy())  ##error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "[1. 2. 3.]\n",
      "tensor([2., 2., 3.], requires_grad=True)\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "a = x.detach()\n",
    "print(a)\n",
    "print(a.numpy())\n",
    "\n",
    "a[0] = 2.\n",
    "print(x)\n",
    "\n",
    "print(x.requires_grad)\n",
    "print(a.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 3.], requires_grad=True)\n",
      "tensor([3., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "b = x.clone().detach()\n",
    "b[0] = 3.\n",
    "print(x)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 3.])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# in-place detach\n",
    "x.detach_().numpy()\n",
    "print(x)\n",
    "print(x.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a shallow net - Numpy vs Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -------> 49296443.61824\n",
      "1 -------> 54187626.30186035\n",
      "2 -------> 52586434.939787515\n",
      "3 -------> 36840957.31626769\n",
      "4 -------> 18025770.103716694\n",
      "5 -------> 7294451.621323355\n",
      "6 -------> 3389683.4928840627\n",
      "7 -------> 2074772.775212025\n",
      "8 -------> 1528188.5100554563\n",
      "9 -------> 1218788.2430667912\n",
      "10 -------> 1003578.9947752419\n",
      "11 -------> 839410.6453235886\n",
      "12 -------> 709449.2272592331\n",
      "13 -------> 604267.1866932148\n",
      "14 -------> 518131.0484267704\n",
      "15 -------> 446982.92873274244\n",
      "16 -------> 387684.06467675767\n",
      "17 -------> 337898.4524261534\n",
      "18 -------> 295837.16935249546\n",
      "19 -------> 260081.0034696145\n",
      "20 -------> 229510.89479177853\n",
      "21 -------> 203312.26406422863\n",
      "22 -------> 180697.36664177914\n",
      "23 -------> 161092.9656963374\n",
      "24 -------> 144042.71415889254\n",
      "25 -------> 129156.43170987215\n",
      "26 -------> 116100.11129293067\n",
      "27 -------> 104605.16452378513\n",
      "28 -------> 94450.06030034294\n",
      "29 -------> 85458.56560070353\n",
      "30 -------> 77466.38624705595\n",
      "31 -------> 70346.04556575735\n",
      "32 -------> 63989.33457531284\n",
      "33 -------> 58302.26861639771\n",
      "34 -------> 53200.717286911575\n",
      "35 -------> 48618.644854668455\n",
      "36 -------> 44489.993739997015\n",
      "37 -------> 40764.85044288488\n",
      "38 -------> 37398.87697013924\n",
      "39 -------> 34348.89206120766\n",
      "40 -------> 31584.0146839772\n",
      "41 -------> 29072.51243062158\n",
      "42 -------> 26786.59903588526\n",
      "43 -------> 24703.85454399424\n",
      "44 -------> 22804.730437586008\n",
      "45 -------> 21070.52527455752\n",
      "46 -------> 19484.20044752031\n",
      "47 -------> 18031.228103460104\n",
      "48 -------> 16698.49050191709\n",
      "49 -------> 15475.909822689691\n",
      "50 -------> 14353.985967264667\n",
      "51 -------> 13321.78308629367\n",
      "52 -------> 12371.747330682909\n",
      "53 -------> 11496.91601191388\n",
      "54 -------> 10690.655197105916\n",
      "55 -------> 9946.75186022414\n",
      "56 -------> 9259.99217566439\n",
      "57 -------> 8625.14409531702\n",
      "58 -------> 8038.073827808506\n",
      "59 -------> 7494.907066289486\n",
      "60 -------> 6992.0889682095085\n",
      "61 -------> 6526.053600158013\n",
      "62 -------> 6094.242904345232\n",
      "63 -------> 5693.257647233856\n",
      "64 -------> 5321.043026040567\n",
      "65 -------> 4975.395078194299\n",
      "66 -------> 4654.1410849253225\n",
      "67 -------> 4355.674896589677\n",
      "68 -------> 4077.9954945712702\n",
      "69 -------> 3819.5968155363526\n",
      "70 -------> 3578.964124195154\n",
      "71 -------> 3354.5082803620826\n",
      "72 -------> 3145.2029623858593\n",
      "73 -------> 2950.3390703837244\n",
      "74 -------> 2768.7621195020392\n",
      "75 -------> 2599.1482024466236\n",
      "76 -------> 2440.8297780150915\n",
      "77 -------> 2292.7968156091215\n",
      "78 -------> 2154.3647540160428\n",
      "79 -------> 2024.8984205114987\n",
      "80 -------> 1903.885525235124\n",
      "81 -------> 1790.561949311916\n",
      "82 -------> 1684.4557295983143\n",
      "83 -------> 1585.0101591372716\n",
      "84 -------> 1491.8719082744515\n",
      "85 -------> 1404.5361947913323\n",
      "86 -------> 1322.6239574407177\n",
      "87 -------> 1245.7995101136503\n",
      "88 -------> 1173.7131823264986\n",
      "89 -------> 1106.0576903849178\n",
      "90 -------> 1042.541303973733\n",
      "91 -------> 982.9202114854522\n",
      "92 -------> 926.9020400185543\n",
      "93 -------> 874.2492447465436\n",
      "94 -------> 824.7447607366244\n",
      "95 -------> 778.218213330741\n",
      "96 -------> 734.4527919360528\n",
      "97 -------> 693.2725633491395\n",
      "98 -------> 654.5243968808084\n",
      "99 -------> 618.0690042379482\n",
      "100 -------> 583.7509420017623\n",
      "101 -------> 551.440046904189\n",
      "102 -------> 521.0378834027135\n",
      "103 -------> 492.36874533372804\n",
      "104 -------> 465.39076942375027\n",
      "105 -------> 439.97865777355355\n",
      "106 -------> 416.0066127221331\n",
      "107 -------> 393.40135735176034\n",
      "108 -------> 372.08143429346586\n",
      "109 -------> 351.96917359345036\n",
      "110 -------> 332.99712195972427\n",
      "111 -------> 315.0904037442486\n",
      "112 -------> 298.203636740282\n",
      "113 -------> 282.25812104777\n",
      "114 -------> 267.1959125746888\n",
      "115 -------> 252.97115449187277\n",
      "116 -------> 239.53833129775978\n",
      "117 -------> 226.84175756374077\n",
      "118 -------> 214.84459579248022\n",
      "119 -------> 203.50933727083634\n",
      "120 -------> 192.7961456238665\n",
      "121 -------> 182.67103106152598\n",
      "122 -------> 173.0976244906601\n",
      "123 -------> 164.0470754910103\n",
      "124 -------> 155.48515651555422\n",
      "125 -------> 147.38523188443247\n",
      "126 -------> 139.71968893258514\n",
      "127 -------> 132.4676294862596\n",
      "128 -------> 125.604532981891\n",
      "129 -------> 119.11099600334217\n",
      "130 -------> 112.96379522056338\n",
      "131 -------> 107.14508698421665\n",
      "132 -------> 101.637361804544\n",
      "133 -------> 96.42254625250399\n",
      "134 -------> 91.48233051008427\n",
      "135 -------> 86.80444579989094\n",
      "136 -------> 82.3730287753576\n",
      "137 -------> 78.17301241347867\n",
      "138 -------> 74.1938008867388\n",
      "139 -------> 70.42500259404325\n",
      "140 -------> 66.85238347489523\n",
      "141 -------> 63.46537810537382\n",
      "142 -------> 60.255608310069235\n",
      "143 -------> 57.21320967573332\n",
      "144 -------> 54.32803236414641\n",
      "145 -------> 51.59346426107125\n",
      "146 -------> 48.99957817685138\n",
      "147 -------> 46.54042668889785\n",
      "148 -------> 44.20754414624815\n",
      "149 -------> 41.99399137449578\n",
      "150 -------> 39.89433209013572\n",
      "151 -------> 37.90222370920573\n",
      "152 -------> 36.01222765685726\n",
      "153 -------> 34.21916317473239\n",
      "154 -------> 32.51762400448228\n",
      "155 -------> 30.902771594770062\n",
      "156 -------> 29.370439780092408\n",
      "157 -------> 27.91591568524599\n",
      "158 -------> 26.535068482878998\n",
      "159 -------> 25.223851002440256\n",
      "160 -------> 23.97862108316517\n",
      "161 -------> 22.79630334599824\n",
      "162 -------> 21.67361747854921\n",
      "163 -------> 20.607782965523306\n",
      "164 -------> 19.595166857722422\n",
      "165 -------> 18.63342771546167\n",
      "166 -------> 17.719960135588934\n",
      "167 -------> 16.852102179156663\n",
      "168 -------> 16.0276334220752\n",
      "169 -------> 15.244916344991964\n",
      "170 -------> 14.501131133699026\n",
      "171 -------> 13.793909421220079\n",
      "172 -------> 13.122001359003796\n",
      "173 -------> 12.48374799671313\n",
      "174 -------> 11.87684049133687\n",
      "175 -------> 11.300005805111255\n",
      "176 -------> 10.751703658769255\n",
      "177 -------> 10.230541726991849\n",
      "178 -------> 9.735108301411941\n",
      "179 -------> 9.264177312938626\n",
      "180 -------> 8.8165026750734\n",
      "181 -------> 8.390926151184965\n",
      "182 -------> 7.986169275613552\n",
      "183 -------> 7.601288407733154\n",
      "184 -------> 7.235236026283528\n",
      "185 -------> 6.887079139507356\n",
      "186 -------> 6.555949953332733\n",
      "187 -------> 6.241005416960704\n",
      "188 -------> 5.941476006112876\n",
      "189 -------> 5.656672832148707\n",
      "190 -------> 5.385736161574363\n",
      "191 -------> 5.127915365847911\n",
      "192 -------> 4.882734740762515\n",
      "193 -------> 4.649415169695841\n",
      "194 -------> 4.427419834399986\n",
      "195 -------> 4.216148675839009\n",
      "196 -------> 4.01511547735519\n",
      "197 -------> 3.8238565925474823\n",
      "198 -------> 3.641825616355449\n",
      "199 -------> 3.46857925614494\n",
      "200 -------> 3.3036843443126016\n",
      "201 -------> 3.1467366141418314\n",
      "202 -------> 2.9973706971716374\n",
      "203 -------> 2.8552625085582304\n",
      "204 -------> 2.719938755744507\n",
      "205 -------> 2.591163331232649\n",
      "206 -------> 2.468562449444301\n",
      "207 -------> 2.351814668116514\n",
      "208 -------> 2.2406539999886705\n",
      "209 -------> 2.134815940962614\n",
      "210 -------> 2.034056952620822\n",
      "211 -------> 1.9381316400376818\n",
      "212 -------> 1.846780398548432\n",
      "213 -------> 1.7597796985748009\n",
      "214 -------> 1.6769746250318196\n",
      "215 -------> 1.5980766579024064\n",
      "216 -------> 1.5229570432668584\n",
      "217 -------> 1.4514036324294226\n",
      "218 -------> 1.3832433174111636\n",
      "219 -------> 1.3183275825349963\n",
      "220 -------> 1.256497914819371\n",
      "221 -------> 1.1976367810866109\n",
      "222 -------> 1.1415428477879812\n",
      "223 -------> 1.0880992366984161\n",
      "224 -------> 1.0371927770156053\n",
      "225 -------> 0.988709276065635\n",
      "226 -------> 0.9425119866146597\n",
      "227 -------> 0.8984950005055798\n",
      "228 -------> 0.8565586853890653\n",
      "229 -------> 0.8165992528495267\n",
      "230 -------> 0.7785217712658173\n",
      "231 -------> 0.7422386747062888\n",
      "232 -------> 0.7076719344697464\n",
      "233 -------> 0.6747285092905992\n",
      "234 -------> 0.6433359723020127\n",
      "235 -------> 0.6134289607779864\n",
      "236 -------> 0.5849347812433159\n",
      "237 -------> 0.5577714346538426\n",
      "238 -------> 0.5318754162515396\n",
      "239 -------> 0.5071919303941593\n",
      "240 -------> 0.48366859929888173\n",
      "241 -------> 0.46125017780457245\n",
      "242 -------> 0.439879996995918\n",
      "243 -------> 0.4195080804832656\n",
      "244 -------> 0.40009031994719824\n",
      "245 -------> 0.38158121391901906\n",
      "246 -------> 0.3639383812182438\n",
      "247 -------> 0.34712356987138915\n",
      "248 -------> 0.3310866633674402\n",
      "249 -------> 0.3157975593164005\n",
      "250 -------> 0.3012214124753191\n",
      "251 -------> 0.28732442974657385\n",
      "252 -------> 0.2740780745811416\n",
      "253 -------> 0.26144972915020726\n",
      "254 -------> 0.24940607069759252\n",
      "255 -------> 0.2379207357125167\n",
      "256 -------> 0.22696866593421688\n",
      "257 -------> 0.21652780784902506\n",
      "258 -------> 0.2065716874018938\n",
      "259 -------> 0.19707781002684682\n",
      "260 -------> 0.18802169269910013\n",
      "261 -------> 0.17938595085018458\n",
      "262 -------> 0.17115099583348564\n",
      "263 -------> 0.1632967015605472\n",
      "264 -------> 0.15580630543579282\n",
      "265 -------> 0.14866207264354167\n",
      "266 -------> 0.1418477850561321\n",
      "267 -------> 0.1353489513017871\n",
      "268 -------> 0.12915446588400015\n",
      "269 -------> 0.12324179373275561\n",
      "270 -------> 0.11760332651612494\n",
      "271 -------> 0.11222448571502351\n",
      "272 -------> 0.10709266569896708\n",
      "273 -------> 0.10219793253865463\n",
      "274 -------> 0.09752845453129264\n",
      "275 -------> 0.09307410205100317\n",
      "276 -------> 0.08882520150527104\n",
      "277 -------> 0.08477144852549814\n",
      "278 -------> 0.08090518554834741\n",
      "279 -------> 0.07721633059614366\n",
      "280 -------> 0.07369623495852161\n",
      "281 -------> 0.0703378402482249\n",
      "282 -------> 0.06713392698572529\n",
      "283 -------> 0.06407770884518563\n",
      "284 -------> 0.061160950455769185\n",
      "285 -------> 0.0583781468117963\n",
      "286 -------> 0.05572263142753096\n",
      "287 -------> 0.05318875774416048\n",
      "288 -------> 0.050770938938405466\n",
      "289 -------> 0.048464565827689196\n",
      "290 -------> 0.046262917588834385\n",
      "291 -------> 0.04416192798024366\n",
      "292 -------> 0.04215736326982035\n",
      "293 -------> 0.04024457028959498\n",
      "294 -------> 0.03841898458132862\n",
      "295 -------> 0.03667667443201576\n",
      "296 -------> 0.03501396819814693\n",
      "297 -------> 0.033427067722540366\n",
      "298 -------> 0.03191306883847481\n",
      "299 -------> 0.030468192415476058\n",
      "300 -------> 0.029088965563547405\n",
      "301 -------> 0.02777230540211\n",
      "302 -------> 0.026515671166799433\n",
      "303 -------> 0.025316181690573415\n",
      "304 -------> 0.024171368929066645\n",
      "305 -------> 0.023078687366157667\n",
      "306 -------> 0.022035800272067106\n",
      "307 -------> 0.021040312804336873\n",
      "308 -------> 0.020089982110087584\n",
      "309 -------> 0.019183018309320995\n",
      "310 -------> 0.01831725948147454\n",
      "311 -------> 0.017490640592596408\n",
      "312 -------> 0.01670157424209476\n",
      "313 -------> 0.01594874122231485\n",
      "314 -------> 0.015229742536434592\n",
      "315 -------> 0.014543296311532429\n",
      "316 -------> 0.013888012089327974\n",
      "317 -------> 0.0132624532737325\n",
      "318 -------> 0.012665148532850697\n",
      "319 -------> 0.01209499843210204\n",
      "320 -------> 0.01155083697896326\n",
      "321 -------> 0.011031062671100472\n",
      "322 -------> 0.010534804568034427\n",
      "323 -------> 0.010061040859316958\n",
      "324 -------> 0.009608691522169165\n",
      "325 -------> 0.009176841355389199\n",
      "326 -------> 0.008764524556196278\n",
      "327 -------> 0.008370814315263868\n",
      "328 -------> 0.007994965580876453\n",
      "329 -------> 0.0076359793553286434\n",
      "330 -------> 0.0072933041907233985\n",
      "331 -------> 0.00696601506139114\n",
      "332 -------> 0.00665351939588325\n",
      "333 -------> 0.006355083193268901\n",
      "334 -------> 0.006070132163616378\n",
      "335 -------> 0.005798001952378626\n",
      "336 -------> 0.005538138498605341\n",
      "337 -------> 0.005289977320160209\n",
      "338 -------> 0.005053047225183753\n",
      "339 -------> 0.004826740877058579\n",
      "340 -------> 0.004610672282426854\n",
      "341 -------> 0.004404296222123769\n",
      "342 -------> 0.004207217188943764\n",
      "343 -------> 0.004019026784150231\n",
      "344 -------> 0.0038392799676472047\n",
      "345 -------> 0.003667580874095491\n",
      "346 -------> 0.003503604312077621\n",
      "347 -------> 0.0033469938174957186\n",
      "348 -------> 0.003197417665054915\n",
      "349 -------> 0.0030545679814838068\n",
      "350 -------> 0.0029181716332093064\n",
      "351 -------> 0.0027878556455493836\n",
      "352 -------> 0.0026633807024236266\n",
      "353 -------> 0.002544496629829521\n",
      "354 -------> 0.0024309427196968878\n",
      "355 -------> 0.0023224973892982375\n",
      "356 -------> 0.002218899830276705\n",
      "357 -------> 0.002119951741127366\n",
      "358 -------> 0.002025450073572164\n",
      "359 -------> 0.001935156905531709\n",
      "360 -------> 0.0018489248982330123\n",
      "361 -------> 0.0017665594128516379\n",
      "362 -------> 0.0016878687712392083\n",
      "363 -------> 0.0016126888904000624\n",
      "364 -------> 0.0015408747277146893\n",
      "365 -------> 0.0014722724647140948\n",
      "366 -------> 0.0014067581211762701\n",
      "367 -------> 0.0013441572457801448\n",
      "368 -------> 0.001284352435063153\n",
      "369 -------> 0.0012272224503406845\n",
      "370 -------> 0.0011726522817208974\n",
      "371 -------> 0.0011205206856915529\n",
      "372 -------> 0.0010707213232637814\n",
      "373 -------> 0.0010231366075053312\n",
      "374 -------> 0.0009776717324278864\n",
      "375 -------> 0.0009342354583731078\n",
      "376 -------> 0.0008927397781656728\n",
      "377 -------> 0.0008530985090144626\n",
      "378 -------> 0.0008152199688469293\n",
      "379 -------> 0.000779030998595035\n",
      "380 -------> 0.0007444567021819467\n",
      "381 -------> 0.0007114246751227673\n",
      "382 -------> 0.0006798663848197912\n",
      "383 -------> 0.0006497109034395889\n",
      "384 -------> 0.000620895766620668\n",
      "385 -------> 0.0005933637595985698\n",
      "386 -------> 0.0005670589880222906\n",
      "387 -------> 0.0005419364960805903\n",
      "388 -------> 0.0005179206954685983\n",
      "389 -------> 0.000494972551192715\n",
      "390 -------> 0.00047304699192032493\n",
      "391 -------> 0.0004520980129134206\n",
      "392 -------> 0.00043208158756281444\n",
      "393 -------> 0.0004129512452820936\n",
      "394 -------> 0.0003946712404987781\n",
      "395 -------> 0.0003772037767023706\n",
      "396 -------> 0.00036051278815127575\n",
      "397 -------> 0.00034456670919281855\n",
      "398 -------> 0.0003293250927364396\n",
      "399 -------> 0.00031475969391133704\n",
      "400 -------> 0.0003008412298389196\n",
      "401 -------> 0.00028754514450380936\n",
      "402 -------> 0.00027483958988654275\n",
      "403 -------> 0.00026269280442875844\n",
      "404 -------> 0.0002510848321147582\n",
      "405 -------> 0.00023999230499642906\n",
      "406 -------> 0.0002293915838908027\n",
      "407 -------> 0.00021926191422292306\n",
      "408 -------> 0.0002095796657481084\n",
      "409 -------> 0.00020032674837110735\n",
      "410 -------> 0.00019148435854573142\n",
      "411 -------> 0.00018303467399189707\n",
      "412 -------> 0.00017495942198324173\n",
      "413 -------> 0.0001672400351429444\n",
      "414 -------> 0.0001598625249938302\n",
      "415 -------> 0.00015281182421165716\n",
      "416 -------> 0.0001460755425068629\n",
      "417 -------> 0.00013963559634758068\n",
      "418 -------> 0.00013347981168437497\n",
      "419 -------> 0.00012759662018158632\n",
      "420 -------> 0.00012197365531269973\n",
      "421 -------> 0.00011660053419925353\n",
      "422 -------> 0.00011146401805671671\n",
      "423 -------> 0.00010655417060514654\n",
      "424 -------> 0.0001018616391188165\n",
      "425 -------> 9.737657913782332e-05\n",
      "426 -------> 9.30897517000007e-05\n",
      "427 -------> 8.899199953530011e-05\n",
      "428 -------> 8.50750756935301e-05\n",
      "429 -------> 8.133124699422019e-05\n",
      "430 -------> 7.775396437816962e-05\n",
      "431 -------> 7.433376505128146e-05\n",
      "432 -------> 7.106448029194278e-05\n",
      "433 -------> 6.793903354863826e-05\n",
      "434 -------> 6.495187133975273e-05\n",
      "435 -------> 6.209656907019693e-05\n",
      "436 -------> 5.936676605874602e-05\n",
      "437 -------> 5.675746948231352e-05\n",
      "438 -------> 5.4263268806034254e-05\n",
      "439 -------> 5.187938370544942e-05\n",
      "440 -------> 4.96001588222453e-05\n",
      "441 -------> 4.742133117134884e-05\n",
      "442 -------> 4.533882304846164e-05\n",
      "443 -------> 4.334806661312824e-05\n",
      "444 -------> 4.144532201336558e-05\n",
      "445 -------> 3.962598820578114e-05\n",
      "446 -------> 3.788658010478636e-05\n",
      "447 -------> 3.6223790777454594e-05\n",
      "448 -------> 3.4634475056584776e-05\n",
      "449 -------> 3.311487953584085e-05\n",
      "450 -------> 3.166220456500053e-05\n",
      "451 -------> 3.0273388037779673e-05\n",
      "452 -------> 2.8945902294069486e-05\n",
      "453 -------> 2.7676766043652744e-05\n",
      "454 -------> 2.646328037943319e-05\n",
      "455 -------> 2.5303153909736648e-05\n",
      "456 -------> 2.419412446889156e-05\n",
      "457 -------> 2.313390749416282e-05\n",
      "458 -------> 2.2120288315288095e-05\n",
      "459 -------> 2.11512192880038e-05\n",
      "460 -------> 2.0224644729714578e-05\n",
      "461 -------> 1.9338873521202173e-05\n",
      "462 -------> 1.8491879034529943e-05\n",
      "463 -------> 1.7682187298670162e-05\n",
      "464 -------> 1.690799068249675e-05\n",
      "465 -------> 1.6167935544708688e-05\n",
      "466 -------> 1.5460227307828438e-05\n",
      "467 -------> 1.478355861368742e-05\n",
      "468 -------> 1.4136597346876026e-05\n",
      "469 -------> 1.351812926433405e-05\n",
      "470 -------> 1.292673488156961e-05\n",
      "471 -------> 1.236126099274452e-05\n",
      "472 -------> 1.182062972725792e-05\n",
      "473 -------> 1.1303830174677205e-05\n",
      "474 -------> 1.0809637265035224e-05\n",
      "475 -------> 1.0337025619002917e-05\n",
      "476 -------> 9.885135400133977e-06\n",
      "477 -------> 9.453084767330983e-06\n",
      "478 -------> 9.03996923609653e-06\n",
      "479 -------> 8.644925988611398e-06\n",
      "480 -------> 8.267193889599494e-06\n",
      "481 -------> 7.90605540630087e-06\n",
      "482 -------> 7.560739720861589e-06\n",
      "483 -------> 7.230499771782091e-06\n",
      "484 -------> 6.914738740079404e-06\n",
      "485 -------> 6.612838017285976e-06\n",
      "486 -------> 6.324163311546206e-06\n",
      "487 -------> 6.04813053571522e-06\n",
      "488 -------> 5.784122571789947e-06\n",
      "489 -------> 5.531689316983637e-06\n",
      "490 -------> 5.29031943226026e-06\n",
      "491 -------> 5.059478472635745e-06\n",
      "492 -------> 4.8387422615072175e-06\n",
      "493 -------> 4.6276719888817564e-06\n",
      "494 -------> 4.425840802617533e-06\n",
      "495 -------> 4.232824848768059e-06\n",
      "496 -------> 4.048247297540067e-06\n",
      "497 -------> 3.871744830581578e-06\n",
      "498 -------> 3.702972664704621e-06\n",
      "499 -------> 3.5415512590733658e-06\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t,\"------->\", loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -------> 31917524.0\n",
      "1 -------> 29315808.0\n",
      "2 -------> 28159078.0\n",
      "3 -------> 24961372.0\n",
      "4 -------> 18918446.0\n",
      "5 -------> 12358803.0\n",
      "6 -------> 7222443.0\n",
      "7 -------> 4123861.0\n",
      "8 -------> 2453943.25\n",
      "9 -------> 1590925.625\n",
      "10 -------> 1126923.75\n",
      "11 -------> 857272.5625\n",
      "12 -------> 684531.0\n",
      "13 -------> 563333.5625\n",
      "14 -------> 472434.75\n",
      "15 -------> 401172.46875\n",
      "16 -------> 343531.8125\n",
      "17 -------> 296137.5\n",
      "18 -------> 256636.34375\n",
      "19 -------> 223400.8125\n",
      "20 -------> 195276.609375\n",
      "21 -------> 171306.984375\n",
      "22 -------> 150774.5\n",
      "23 -------> 133102.328125\n",
      "24 -------> 117842.2109375\n",
      "25 -------> 104624.109375\n",
      "26 -------> 93139.6328125\n",
      "27 -------> 83115.71875\n",
      "28 -------> 74341.8203125\n",
      "29 -------> 66644.03125\n",
      "30 -------> 59864.41796875\n",
      "31 -------> 53880.03125\n",
      "32 -------> 48579.9453125\n",
      "33 -------> 43875.71875\n",
      "34 -------> 39694.40625\n",
      "35 -------> 35975.34375\n",
      "36 -------> 32652.630859375\n",
      "37 -------> 29679.46875\n",
      "38 -------> 27017.556640625\n",
      "39 -------> 24626.3828125\n",
      "40 -------> 22477.0\n",
      "41 -------> 20541.291015625\n",
      "42 -------> 18793.455078125\n",
      "43 -------> 17213.95703125\n",
      "44 -------> 15783.888671875\n",
      "45 -------> 14487.390625\n",
      "46 -------> 13310.6376953125\n",
      "47 -------> 12241.3271484375\n",
      "48 -------> 11268.224609375\n",
      "49 -------> 10382.767578125\n",
      "50 -------> 9574.806640625\n",
      "51 -------> 8837.1455078125\n",
      "52 -------> 8162.56640625\n",
      "53 -------> 7545.6484375\n",
      "54 -------> 6981.01904296875\n",
      "55 -------> 6463.2412109375\n",
      "56 -------> 5988.39453125\n",
      "57 -------> 5552.2451171875\n",
      "58 -------> 5154.927734375\n",
      "59 -------> 4789.0869140625\n",
      "60 -------> 4451.88134765625\n",
      "61 -------> 4140.970703125\n",
      "62 -------> 3854.061767578125\n",
      "63 -------> 3589.18359375\n",
      "64 -------> 3344.131103515625\n",
      "65 -------> 3117.47216796875\n",
      "66 -------> 2907.64453125\n",
      "67 -------> 2713.269287109375\n",
      "68 -------> 2533.0859375\n",
      "69 -------> 2365.89501953125\n",
      "70 -------> 2210.723876953125\n",
      "71 -------> 2066.5966796875\n",
      "72 -------> 1932.6829833984375\n",
      "73 -------> 1808.125244140625\n",
      "74 -------> 1692.28955078125\n",
      "75 -------> 1584.4654541015625\n",
      "76 -------> 1484.02783203125\n",
      "77 -------> 1390.4752197265625\n",
      "78 -------> 1303.236572265625\n",
      "79 -------> 1221.8934326171875\n",
      "80 -------> 1145.97998046875\n",
      "81 -------> 1075.1351318359375\n",
      "82 -------> 1008.97412109375\n",
      "83 -------> 947.1541748046875\n",
      "84 -------> 889.3826904296875\n",
      "85 -------> 835.3676147460938\n",
      "86 -------> 784.8521728515625\n",
      "87 -------> 737.56103515625\n",
      "88 -------> 693.306640625\n",
      "89 -------> 651.8864135742188\n",
      "90 -------> 613.0725708007812\n",
      "91 -------> 576.711181640625\n",
      "92 -------> 542.6340942382812\n",
      "93 -------> 510.67816162109375\n",
      "94 -------> 480.710205078125\n",
      "95 -------> 452.5945129394531\n",
      "96 -------> 426.22314453125\n",
      "97 -------> 401.4621276855469\n",
      "98 -------> 378.20880126953125\n",
      "99 -------> 356.37591552734375\n",
      "100 -------> 335.86688232421875\n",
      "101 -------> 316.5928955078125\n",
      "102 -------> 298.47698974609375\n",
      "103 -------> 281.44952392578125\n",
      "104 -------> 265.4385070800781\n",
      "105 -------> 250.38279724121094\n",
      "106 -------> 236.21414184570312\n",
      "107 -------> 222.88388061523438\n",
      "108 -------> 210.3376007080078\n",
      "109 -------> 198.52880859375\n",
      "110 -------> 187.413330078125\n",
      "111 -------> 176.9443359375\n",
      "112 -------> 167.08474731445312\n",
      "113 -------> 157.7956085205078\n",
      "114 -------> 149.04283142089844\n",
      "115 -------> 140.79408264160156\n",
      "116 -------> 133.01828002929688\n",
      "117 -------> 125.68860626220703\n",
      "118 -------> 118.77783203125\n",
      "119 -------> 112.25862884521484\n",
      "120 -------> 106.11172485351562\n",
      "121 -------> 100.3130874633789\n",
      "122 -------> 94.84172058105469\n",
      "123 -------> 89.67942810058594\n",
      "124 -------> 84.8073501586914\n",
      "125 -------> 80.2083740234375\n",
      "126 -------> 75.8672103881836\n",
      "127 -------> 71.76669311523438\n",
      "128 -------> 67.89668273925781\n",
      "129 -------> 64.24180603027344\n",
      "130 -------> 60.78921127319336\n",
      "131 -------> 57.52784729003906\n",
      "132 -------> 54.44676971435547\n",
      "133 -------> 51.535118103027344\n",
      "134 -------> 48.78449249267578\n",
      "135 -------> 46.184295654296875\n",
      "136 -------> 43.724998474121094\n",
      "137 -------> 41.4023323059082\n",
      "138 -------> 39.20512390136719\n",
      "139 -------> 37.12928771972656\n",
      "140 -------> 35.16472244262695\n",
      "141 -------> 33.306610107421875\n",
      "142 -------> 31.550790786743164\n",
      "143 -------> 29.889270782470703\n",
      "144 -------> 28.316621780395508\n",
      "145 -------> 26.828933715820312\n",
      "146 -------> 25.421775817871094\n",
      "147 -------> 24.090373992919922\n",
      "148 -------> 22.830289840698242\n",
      "149 -------> 21.637630462646484\n",
      "150 -------> 20.50892448425293\n",
      "151 -------> 19.43984603881836\n",
      "152 -------> 18.427780151367188\n",
      "153 -------> 17.469749450683594\n",
      "154 -------> 16.56222152709961\n",
      "155 -------> 15.703459739685059\n",
      "156 -------> 14.890117645263672\n",
      "157 -------> 14.11972427368164\n",
      "158 -------> 13.389602661132812\n",
      "159 -------> 12.698638916015625\n",
      "160 -------> 12.044280052185059\n",
      "161 -------> 11.423455238342285\n",
      "162 -------> 10.836007118225098\n",
      "163 -------> 10.279142379760742\n",
      "164 -------> 9.751249313354492\n",
      "165 -------> 9.251058578491211\n",
      "166 -------> 8.777361869812012\n",
      "167 -------> 8.3280029296875\n",
      "168 -------> 7.9022111892700195\n",
      "169 -------> 7.498312473297119\n",
      "170 -------> 7.115723133087158\n",
      "171 -------> 6.753114700317383\n",
      "172 -------> 6.408815860748291\n",
      "173 -------> 6.083035469055176\n",
      "174 -------> 5.773592948913574\n",
      "175 -------> 5.479973316192627\n",
      "176 -------> 5.202027320861816\n",
      "177 -------> 4.938370227813721\n",
      "178 -------> 4.688321113586426\n",
      "179 -------> 4.450956344604492\n",
      "180 -------> 4.225763320922852\n",
      "181 -------> 4.012117385864258\n",
      "182 -------> 3.8097946643829346\n",
      "183 -------> 3.6178066730499268\n",
      "184 -------> 3.4356706142425537\n",
      "185 -------> 3.262585401535034\n",
      "186 -------> 3.0983052253723145\n",
      "187 -------> 2.9426674842834473\n",
      "188 -------> 2.794672966003418\n",
      "189 -------> 2.6543853282928467\n",
      "190 -------> 2.5212645530700684\n",
      "191 -------> 2.3949759006500244\n",
      "192 -------> 2.2749762535095215\n",
      "193 -------> 2.1610512733459473\n",
      "194 -------> 2.0530195236206055\n",
      "195 -------> 1.9505643844604492\n",
      "196 -------> 1.8529268503189087\n",
      "197 -------> 1.7606699466705322\n",
      "198 -------> 1.6728920936584473\n",
      "199 -------> 1.5895723104476929\n",
      "200 -------> 1.5104173421859741\n",
      "201 -------> 1.4352076053619385\n",
      "202 -------> 1.3637281656265259\n",
      "203 -------> 1.2960549592971802\n",
      "204 -------> 1.2317099571228027\n",
      "205 -------> 1.170539379119873\n",
      "206 -------> 1.1125273704528809\n",
      "207 -------> 1.057417631149292\n",
      "208 -------> 1.0050809383392334\n",
      "209 -------> 0.9553576111793518\n",
      "210 -------> 0.9081484079360962\n",
      "211 -------> 0.863290548324585\n",
      "212 -------> 0.8205443620681763\n",
      "213 -------> 0.7800513505935669\n",
      "214 -------> 0.7415518760681152\n",
      "215 -------> 0.7049962282180786\n",
      "216 -------> 0.6702041029930115\n",
      "217 -------> 0.6372244954109192\n",
      "218 -------> 0.6059391498565674\n",
      "219 -------> 0.5760563015937805\n",
      "220 -------> 0.5477526187896729\n",
      "221 -------> 0.520798921585083\n",
      "222 -------> 0.4952888786792755\n",
      "223 -------> 0.47094422578811646\n",
      "224 -------> 0.44785547256469727\n",
      "225 -------> 0.42587825655937195\n",
      "226 -------> 0.40502268075942993\n",
      "227 -------> 0.3851882815361023\n",
      "228 -------> 0.36630234122276306\n",
      "229 -------> 0.3483949899673462\n",
      "230 -------> 0.3312802016735077\n",
      "231 -------> 0.3151572644710541\n",
      "232 -------> 0.29972100257873535\n",
      "233 -------> 0.28509214520454407\n",
      "234 -------> 0.2711962163448334\n",
      "235 -------> 0.25800058245658875\n",
      "236 -------> 0.24540162086486816\n",
      "237 -------> 0.2334292232990265\n",
      "238 -------> 0.22202569246292114\n",
      "239 -------> 0.21121400594711304\n",
      "240 -------> 0.20094193518161774\n",
      "241 -------> 0.19118236005306244\n",
      "242 -------> 0.1818973571062088\n",
      "243 -------> 0.17308618128299713\n",
      "244 -------> 0.16469015181064606\n",
      "245 -------> 0.15666380524635315\n",
      "246 -------> 0.14907142519950867\n",
      "247 -------> 0.141837477684021\n",
      "248 -------> 0.13492393493652344\n",
      "249 -------> 0.12840215861797333\n",
      "250 -------> 0.12220875918865204\n",
      "251 -------> 0.11629313975572586\n",
      "252 -------> 0.1106390431523323\n",
      "253 -------> 0.10530712455511093\n",
      "254 -------> 0.10018104314804077\n",
      "255 -------> 0.09536483883857727\n",
      "256 -------> 0.09074877947568893\n",
      "257 -------> 0.08636675029993057\n",
      "258 -------> 0.08221666514873505\n",
      "259 -------> 0.07825109362602234\n",
      "260 -------> 0.07445929944515228\n",
      "261 -------> 0.0708819031715393\n",
      "262 -------> 0.06745847314596176\n",
      "263 -------> 0.06421750783920288\n",
      "264 -------> 0.06113056465983391\n",
      "265 -------> 0.05816507339477539\n",
      "266 -------> 0.05538669601082802\n",
      "267 -------> 0.05271996930241585\n",
      "268 -------> 0.05019204691052437\n",
      "269 -------> 0.04778767377138138\n",
      "270 -------> 0.0455101877450943\n",
      "271 -------> 0.04332926496863365\n",
      "272 -------> 0.041250500828027725\n",
      "273 -------> 0.03927316889166832\n",
      "274 -------> 0.03737704083323479\n",
      "275 -------> 0.03560274839401245\n",
      "276 -------> 0.03387288376688957\n",
      "277 -------> 0.03228526934981346\n",
      "278 -------> 0.030743837356567383\n",
      "279 -------> 0.0292828269302845\n",
      "280 -------> 0.02788732573390007\n",
      "281 -------> 0.02654412016272545\n",
      "282 -------> 0.025282472372055054\n",
      "283 -------> 0.024085979908704758\n",
      "284 -------> 0.022936686873435974\n",
      "285 -------> 0.021855007857084274\n",
      "286 -------> 0.020829569548368454\n",
      "287 -------> 0.019833283498883247\n",
      "288 -------> 0.01889606937766075\n",
      "289 -------> 0.018014708533883095\n",
      "290 -------> 0.017160112038254738\n",
      "291 -------> 0.016345549374818802\n",
      "292 -------> 0.015583092346787453\n",
      "293 -------> 0.014842589385807514\n",
      "294 -------> 0.014142878353595734\n",
      "295 -------> 0.013478074222803116\n",
      "296 -------> 0.012857046909630299\n",
      "297 -------> 0.01225217618048191\n",
      "298 -------> 0.011676178313791752\n",
      "299 -------> 0.011132802814245224\n",
      "300 -------> 0.010613138787448406\n",
      "301 -------> 0.01011552195996046\n",
      "302 -------> 0.009641383774578571\n",
      "303 -------> 0.0091982027515769\n",
      "304 -------> 0.0087617551907897\n",
      "305 -------> 0.008358904160559177\n",
      "306 -------> 0.007972021587193012\n",
      "307 -------> 0.00760969752445817\n",
      "308 -------> 0.0072590806521475315\n",
      "309 -------> 0.006931347772479057\n",
      "310 -------> 0.006618144456297159\n",
      "311 -------> 0.006318768486380577\n",
      "312 -------> 0.006028029136359692\n",
      "313 -------> 0.005758992396295071\n",
      "314 -------> 0.00550236739218235\n",
      "315 -------> 0.00525444932281971\n",
      "316 -------> 0.005017678719013929\n",
      "317 -------> 0.004794531501829624\n",
      "318 -------> 0.004580347798764706\n",
      "319 -------> 0.004373159259557724\n",
      "320 -------> 0.004180325195193291\n",
      "321 -------> 0.003998402506113052\n",
      "322 -------> 0.003823391627520323\n",
      "323 -------> 0.003654242493212223\n",
      "324 -------> 0.003497416852042079\n",
      "325 -------> 0.003344861092045903\n",
      "326 -------> 0.00319852028042078\n",
      "327 -------> 0.0030624866485595703\n",
      "328 -------> 0.0029288684017956257\n",
      "329 -------> 0.002804119372740388\n",
      "330 -------> 0.0026839689817279577\n",
      "331 -------> 0.0025736489333212376\n",
      "332 -------> 0.002465532161295414\n",
      "333 -------> 0.0023569336626678705\n",
      "334 -------> 0.0022606945130974054\n",
      "335 -------> 0.0021639293991029263\n",
      "336 -------> 0.002074029529467225\n",
      "337 -------> 0.001988685689866543\n",
      "338 -------> 0.0019067316316068172\n",
      "339 -------> 0.0018303308170288801\n",
      "340 -------> 0.0017562417779117823\n",
      "341 -------> 0.001685016555711627\n",
      "342 -------> 0.0016176754143089056\n",
      "343 -------> 0.001556001603603363\n",
      "344 -------> 0.0014942314010113478\n",
      "345 -------> 0.0014348283875733614\n",
      "346 -------> 0.0013792645186185837\n",
      "347 -------> 0.0013255249941721559\n",
      "348 -------> 0.0012742013204842806\n",
      "349 -------> 0.0012269860599189997\n",
      "350 -------> 0.0011791109573096037\n",
      "351 -------> 0.0011366817634552717\n",
      "352 -------> 0.001093763392418623\n",
      "353 -------> 0.0010511163854971528\n",
      "354 -------> 0.001011961605399847\n",
      "355 -------> 0.0009755418286658823\n",
      "356 -------> 0.000940128811635077\n",
      "357 -------> 0.0009079028386622667\n",
      "358 -------> 0.0008741564815863967\n",
      "359 -------> 0.0008430095622316003\n",
      "360 -------> 0.0008119628764688969\n",
      "361 -------> 0.0007858610479161143\n",
      "362 -------> 0.0007572732865810394\n",
      "363 -------> 0.0007306984625756741\n",
      "364 -------> 0.0007078874041326344\n",
      "365 -------> 0.0006823954172432423\n",
      "366 -------> 0.0006583534413948655\n",
      "367 -------> 0.0006387466564774513\n",
      "368 -------> 0.0006179494084790349\n",
      "369 -------> 0.0005984726594761014\n",
      "370 -------> 0.0005785111570730805\n",
      "371 -------> 0.0005600785952992737\n",
      "372 -------> 0.0005421193782240152\n",
      "373 -------> 0.0005253077833913267\n",
      "374 -------> 0.0005084095755591989\n",
      "375 -------> 0.0004925124230794609\n",
      "376 -------> 0.0004775758716277778\n",
      "377 -------> 0.0004633454082068056\n",
      "378 -------> 0.00044859983609057963\n",
      "379 -------> 0.0004343243781477213\n",
      "380 -------> 0.00042140824371017516\n",
      "381 -------> 0.00041027681436389685\n",
      "382 -------> 0.00039737921906635165\n",
      "383 -------> 0.00038593990029767156\n",
      "384 -------> 0.0003750257601495832\n",
      "385 -------> 0.00036471858038567007\n",
      "386 -------> 0.0003546736261341721\n",
      "387 -------> 0.0003447277413215488\n",
      "388 -------> 0.00033493974478915334\n",
      "389 -------> 0.0003253890317864716\n",
      "390 -------> 0.0003157825267408043\n",
      "391 -------> 0.0003064260818064213\n",
      "392 -------> 0.0002985117316711694\n",
      "393 -------> 0.00029106391593813896\n",
      "394 -------> 0.0002833429607562721\n",
      "395 -------> 0.00027667611720971763\n",
      "396 -------> 0.0002692884299904108\n",
      "397 -------> 0.00026210202486254275\n",
      "398 -------> 0.0002541443391237408\n",
      "399 -------> 0.0002482767158653587\n",
      "400 -------> 0.00024243915686383843\n",
      "401 -------> 0.0002367214037803933\n",
      "402 -------> 0.00023095784126780927\n",
      "403 -------> 0.00022557888587471098\n",
      "404 -------> 0.00022035290021449327\n",
      "405 -------> 0.0002148511994164437\n",
      "406 -------> 0.00020935735665261745\n",
      "407 -------> 0.00020452339958865196\n",
      "408 -------> 0.00019977027841378003\n",
      "409 -------> 0.0001956996275112033\n",
      "410 -------> 0.00019083579536527395\n",
      "411 -------> 0.00018612791609484702\n",
      "412 -------> 0.00018205570813734084\n",
      "413 -------> 0.00017805842799134552\n",
      "414 -------> 0.000173783308127895\n",
      "415 -------> 0.00016989563300739974\n",
      "416 -------> 0.0001663619332248345\n",
      "417 -------> 0.00016239253454841673\n",
      "418 -------> 0.00015898843412287533\n",
      "419 -------> 0.00015492145030293614\n",
      "420 -------> 0.000151380110764876\n",
      "421 -------> 0.0001484288804931566\n",
      "422 -------> 0.00014555685629602522\n",
      "423 -------> 0.000142168952152133\n",
      "424 -------> 0.00013944214151706547\n",
      "425 -------> 0.00013619480887427926\n",
      "426 -------> 0.00013315791147761047\n",
      "427 -------> 0.00013024910003878176\n",
      "428 -------> 0.0001276170660275966\n",
      "429 -------> 0.0001253049267688766\n",
      "430 -------> 0.00012272605090402067\n",
      "431 -------> 0.00012054048420395702\n",
      "432 -------> 0.00011824849934782833\n",
      "433 -------> 0.00011593316594371572\n",
      "434 -------> 0.00011392473243176937\n",
      "435 -------> 0.00011138728586956859\n",
      "436 -------> 0.000109253560367506\n",
      "437 -------> 0.00010747854685178027\n",
      "438 -------> 0.00010544015822233632\n",
      "439 -------> 0.00010368157381890342\n",
      "440 -------> 0.00010168190783588216\n",
      "441 -------> 0.0001001104901661165\n",
      "442 -------> 9.814791701501235e-05\n",
      "443 -------> 9.645877435104921e-05\n",
      "444 -------> 9.439697896596044e-05\n",
      "445 -------> 9.288325964007527e-05\n",
      "446 -------> 9.089215018320829e-05\n",
      "447 -------> 8.949422044679523e-05\n",
      "448 -------> 8.792596781859174e-05\n",
      "449 -------> 8.627482748124748e-05\n",
      "450 -------> 8.505502046318725e-05\n",
      "451 -------> 8.362326479982585e-05\n",
      "452 -------> 8.223832992371172e-05\n",
      "453 -------> 8.099684055196121e-05\n",
      "454 -------> 7.952447776915506e-05\n",
      "455 -------> 7.819933671271428e-05\n",
      "456 -------> 7.708399789407849e-05\n",
      "457 -------> 7.561158417956904e-05\n",
      "458 -------> 7.459020707756281e-05\n",
      "459 -------> 7.34147397452034e-05\n",
      "460 -------> 7.260663551278412e-05\n",
      "461 -------> 7.136876229196787e-05\n",
      "462 -------> 7.038436160655692e-05\n",
      "463 -------> 6.929344090167433e-05\n",
      "464 -------> 6.83207981637679e-05\n",
      "465 -------> 6.751839828211814e-05\n",
      "466 -------> 6.62187158013694e-05\n",
      "467 -------> 6.523739284602925e-05\n",
      "468 -------> 6.430943176383153e-05\n",
      "469 -------> 6.369510083459318e-05\n",
      "470 -------> 6.248118006624281e-05\n",
      "471 -------> 6.155546725494787e-05\n",
      "472 -------> 6.055477206245996e-05\n",
      "473 -------> 5.9845995565410703e-05\n",
      "474 -------> 5.9007441450376064e-05\n",
      "475 -------> 5.796207915409468e-05\n",
      "476 -------> 5.702375710825436e-05\n",
      "477 -------> 5.6330012739636004e-05\n",
      "478 -------> 5.5638549383729696e-05\n",
      "479 -------> 5.5043685279088095e-05\n",
      "480 -------> 5.4128526244312525e-05\n",
      "481 -------> 5.3377323638414964e-05\n",
      "482 -------> 5.257063821773045e-05\n",
      "483 -------> 5.200347368372604e-05\n",
      "484 -------> 5.141626752447337e-05\n",
      "485 -------> 5.0652808567974716e-05\n",
      "486 -------> 4.9938582378672436e-05\n",
      "487 -------> 4.9465568736195564e-05\n",
      "488 -------> 4.881261702394113e-05\n",
      "489 -------> 4.8129604692803696e-05\n",
      "490 -------> 4.7451831051148474e-05\n",
      "491 -------> 4.684341911342926e-05\n",
      "492 -------> 4.60708761238493e-05\n",
      "493 -------> 4.555807390715927e-05\n",
      "494 -------> 4.4940134102944285e-05\n",
      "495 -------> 4.453439396456815e-05\n",
      "496 -------> 4.388645538710989e-05\n",
      "497 -------> 4.332776734372601e-05\n",
      "498 -------> 4.299686406739056e-05\n",
      "499 -------> 4.2403582483530045e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "#     if t % 100 == 99:\n",
    "    print(t, \"------->\", loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
